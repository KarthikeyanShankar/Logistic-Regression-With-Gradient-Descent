{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                            Implementation of Logistic Regression using Gradinent Descent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from csv import reader\n",
    "from math import exp\n",
    "import matplotlib.pyplot as plt\n",
    "class LrGd:\n",
    "    def __init__(self,l_rate,epochs,stopping_rule):\n",
    "        self.dataset=[]\n",
    "        self.splits= 2\n",
    "        self.l_rate = l_rate\n",
    "        self.n_epoch = epochs\n",
    "        self.input_file='Auto.data'\n",
    "        self.train_set=[]\n",
    "        self.test_set = []\n",
    "        self.input_attribute=5\n",
    "        self.num_yhat=1\n",
    "        self.weight_and_delta=list()\n",
    "        self.range_low= -0.7\n",
    "        self.range_high = 0.7\n",
    "        self.stopping_rule = stopping_rule\n",
    "    '''Load data from file to list- File contains only the required attributes'''\n",
    "    def load_data_from_csv(self):\n",
    "        with open(self.input_file, 'r') as file:\n",
    "            csv_reader = reader(file)\n",
    "            for row in csv_reader:\n",
    "                self.dataset.append(row)\n",
    "        return self.dataset\n",
    "    '''Strip values and convert it into float'''\n",
    "    def row_to_float(self,column):\n",
    "        for row in self.dataset:\n",
    "            row[column] = float(row[column].strip())\n",
    "\n",
    "    '''Add dummy variables for Origin. i.e., now origin is quantitative with values origin1,origin2'''\n",
    "    def origin_to_quantitatve(self):\n",
    "        '''origin1=1 origin2=0 --> 1\n",
    "           origin1=0 origin2=1 --> 2\n",
    "           origin1=1 origin2=1 --> 3\n",
    "        '''\n",
    "        for row in self.dataset:\n",
    "            if row[-1]==1.0:\n",
    "                del row[-1]\n",
    "                row.append(1)\n",
    "                row.append(0)\n",
    "            elif row[-1]==2.0:\n",
    "                del row[-1]\n",
    "                row.append(0)\n",
    "                row.append(1)\n",
    "            elif row[-1]==3.0:\n",
    "                del row[-1]\n",
    "                row.append(1)\n",
    "                row.append(1)\n",
    "    '''Create a column with 1,0 for mpg and delete mpg from dataset'''\n",
    "    def create_variable_high(self):\n",
    "        for row in self.dataset:\n",
    "            if row[0] >= 23:\n",
    "                row.append(1)\n",
    "                del row[0]\n",
    "            else:\n",
    "                row.append(0)\n",
    "                del row[0]\n",
    "\n",
    "\n",
    "    '''Determine min and max values for each column'''\n",
    "    def dataset_minmax(self):\n",
    "        min_max= [[min(column), max(column)] for column in zip(*self.dataset)]\n",
    "        return min_max\n",
    "\n",
    "    '''Normalize to the range 0-1'''\n",
    "    def normalize_dataset(self,minmax):\n",
    "        # print(len(minmax))\n",
    "        for row in self.dataset:\n",
    "            for i in range(len(row) - 1):\n",
    "                # print(i,\"dsds\",row,len(row))\n",
    "                row[i] = (row[i] - minmax[i][0]) / (minmax[i][1] - minmax[i][0])\n",
    "\n",
    "    '''CV split into 2 equal half'''\n",
    "    def cross_validation_split(self):\n",
    "        dataset_split = list()\n",
    "        dataset_copy = list(self.dataset)\n",
    "        fold_size = int(len(self.dataset) / self.splits)\n",
    "        for i in range(self.splits):\n",
    "            fold = list()\n",
    "            while len(fold) < fold_size:\n",
    "                index = random.randrange(len(dataset_copy))\n",
    "                fold.append(dataset_copy.pop(index))\n",
    "            dataset_split.append(fold)\n",
    "        self.train_set=dataset_split[0]\n",
    "        self.test_set = dataset_split[1]\n",
    "\n",
    "    ''' Initialize the weight of beta with range from -0.7 to 0.7'''\n",
    "    def initial_weight(self):\n",
    "        lo = self.range_low\n",
    "        hi = self.range_high\n",
    "        output_wt= [{'weights': [(hi - lo) * random.random() + lo for i in range(self.input_attribute+ 1)]} for i in\n",
    "                        range(self.num_yhat)]\n",
    "        self.weight_and_delta.append(output_wt)\n",
    "\n",
    "    '''weight implementation for each row'''\n",
    "    def weight_activation(self,beta,row):\n",
    "        activation = beta[-1]\n",
    "        for i in range(len(beta) - 1):\n",
    "            activation += beta[i] * row[i]\n",
    "        return activation\n",
    "\n",
    "    '''Compute the sigmoid for each instance'''\n",
    "    def sigmoid(self,computed_wt):\n",
    "        return 1.0 / (1.0 + exp(-computed_wt))\n",
    "\n",
    "    '''Forward propagation for computing the value of Pi'''\n",
    "    def forward_propagate(self,row):\n",
    "        pi = row\n",
    "        for each in self.weight_and_delta:\n",
    "            # print(layer)\n",
    "            new_inputs = []\n",
    "            for each_beta in each :\n",
    "                # print(neuron,\"wt 1\")\n",
    "                derived_val = self.weight_activation(each_beta['weights'], pi)\n",
    "                each_beta['output'] = self.sigmoid(derived_val)\n",
    "                new_inputs.append(each_beta['output'])\n",
    "            pi= new_inputs\n",
    "        return pi\n",
    "\n",
    "    '''Backward pass and compute di value'''\n",
    "    def BPcoeff(self,pi):\n",
    "        for i in reversed(range(len(self.weight_and_delta))):\n",
    "            each_wt= self.weight_and_delta[i]\n",
    "            errors = list()\n",
    "            if i != len(self.weight_and_delta) - 1:\n",
    "                for j in range(len(each_wt)):\n",
    "                    error = 0.0\n",
    "                    for beta in self.weight_and_delta[i + 1]:\n",
    "                        error += (beta['weights'][j] * beta['delta'])\n",
    "                    errors.append(error)\n",
    "            else:\n",
    "                for j in range(len(each_wt)):\n",
    "                    beta= each_wt[j]\n",
    "                    errors.append(pi[j] - beta['output'])\n",
    "            for j in range(len(each_wt)):\n",
    "                beta= each_wt[j]\n",
    "                beta['delta'] = errors[j] * (beta['output'] * (1.0 * beta['output']))\n",
    "\n",
    "    '''Update weight for each row'''\n",
    "    def update_weights(self,row):\n",
    "        for i in range(len(self.weight_and_delta)):\n",
    "            inputs = row[:-1]\n",
    "            if i != 0:\n",
    "                inputs = [each_instance['output'] for each_instance in self.weight_and_delta[i - 1]]\n",
    "            for di in self.weight_and_delta[i]:\n",
    "                for j in range(len(inputs)):\n",
    "                    try:\n",
    "                        di['weights'][j] += self.l_rate * di['delta'] * inputs[j]\n",
    "                    except:\n",
    "                        pass\n",
    "                di['weights'][-1] += self.l_rate * di['delta']\n",
    "\n",
    "    '''TRAINING MODEL to compute errors for different epochs'''\n",
    "    def train_model(self):\n",
    "        prev_MSE = 0\n",
    "        prev_error_change = 0\n",
    "        for epoch in range(self.n_epoch):\n",
    "            sum_error = 0\n",
    "            for row in self.train_set:\n",
    "                pi = self.forward_propagate(row)\n",
    "                expected = [row[-1]]\n",
    "                # print(self.num_yhat,\"ds\",expected,pi,'ds')\n",
    "                sum_error += sum([(expected[i] - pi[i]) ** 2 for i in range(self.num_yhat)])\n",
    "                # print(sum_error,\"errr\")\n",
    "                self.BPcoeff(expected)\n",
    "                self.update_weights(row)\n",
    "            # print(self.weight_and_delta)\n",
    "            MSE = sum_error / len(self.train_set)\n",
    "            # print('>epoch=%d, lrate=%.3f, error=%.3f' % (epoch, self.l_rate, MSE))\n",
    "            if round(MSE, 3) == prev_MSE:\n",
    "                prev_error_change += 1\n",
    "            else:\n",
    "                prev_error_change = 0\n",
    "            prev_MSE = round(MSE, 3)\n",
    "            # print(MSE, \"ds\", prev_MSE, \"changee\", prev_error_change)\n",
    "            if prev_error_change >= self.stopping_rule:\n",
    "                break\n",
    "    '''Use forward propgation to egt value of Pi for MSE computation'''\n",
    "    def predict(self,row):\n",
    "        outputs = self.forward_propagate(row)\n",
    "        if max(outputs) >= 0.5:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    '''computation of MSE using gradient descent'''\n",
    "    def LRGD(self):\n",
    "        self.initial_weight()\n",
    "        self.train_model()\n",
    "\n",
    "        '''COMPUTE MSE WITH THE OPTIMAL WEIGHT'''\n",
    "        predictions_train = list()\n",
    "        sum_error_test = 0\n",
    "        for row in self.test_set:\n",
    "            predicted_val = self.predict(row)\n",
    "            sum_error_test += sum([(row[-1] - predicted_val) ** 2])\n",
    "        MSE_test=sum_error_test /len(self.test_set)\n",
    "\n",
    "        sum_error_train = 0\n",
    "        for row in self.train_set:\n",
    "            predicted_val = self.predict(row)\n",
    "            sum_error_train += sum([(row[-1] - predicted_val) ** 2])\n",
    "        MSE_train=sum_error_train/len(self.train_set)\n",
    "        '''COMPUTE MSE WITH THE OPTIMAL WEIGHT'''\n",
    "\n",
    "        return MSE_train, MSE_test\n",
    "\n",
    "    def main(self):\n",
    "        self.load_data_from_csv()\n",
    "        self.origin_to_quantitatve()\n",
    "        # Convert data to float\n",
    "        for i in range(len(self.dataset[0])):\n",
    "            self.row_to_float(i)\n",
    "        # if mpg >=23 1 otherwise 0\n",
    "        self.create_variable_high()\n",
    "        # normalize input variables\n",
    "        minmax = self.dataset_minmax()\n",
    "        self.normalize_dataset(minmax)\n",
    "        #CV split\n",
    "        self.cross_validation_split()\n",
    "        #Compute MSE\n",
    "        MSE_train, MSE_test= self.LRGD()\n",
    "        return MSE_train, MSE_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TABULAR DATA of MSE FOR different Epochs and Learning Rate\n",
      "     Learning Rate  Epochs  Test MSE  Train MSE\n",
      "0            0.01     100  0.155779   0.110553\n",
      "1            0.03     500  0.115578   0.105528\n",
      "2            0.07    1000  0.125628   0.105528\n",
      "3            0.09     100  0.120603   0.095477\n",
      "4            0.25     500  0.075377   0.085427\n",
      "5            0.50    1000  0.070352   0.085427\n",
      "6            0.01     100  0.135678   0.095477\n",
      "7            0.03     500  0.105528   0.070352\n",
      "8            0.07    1000  0.130653   0.105528\n",
      "9            0.09     100  0.080402   0.090452\n",
      "10           0.25     500  0.080402   0.100503\n",
      "11           0.50    1000  0.075377   0.140704\n",
      "12           0.01     100  0.100503   0.080402\n",
      "13           0.03     500  0.110553   0.085427\n",
      "14           0.07    1000  0.095477   0.120603\n",
      "15           0.09     100  0.100503   0.080402\n",
      "16           0.25     500  0.100503   0.075377\n",
      "17           0.50    1000  0.120603   0.130653\n"
     ]
    }
   ],
   "source": [
    "#Training the model using Logistic regression and computing the test and train MSE\n",
    "seed_value = 1023  # MMDD from bday\n",
    "random.seed(seed_value)\n",
    "l_rate=[0.01,0.03,0.07,0.09,0.250,0.500]\n",
    "epochs=[100,500,1000]\n",
    "stopping_rule=100\n",
    "test_mse=[]\n",
    "train_mse=[]\n",
    "\n",
    "for each_epoch_val in epochs:\n",
    "    for each_l_rate in l_rate:\n",
    "        gd_object=LrGd(each_l_rate ,each_epoch_val ,stopping_rule)\n",
    "        MSE_train, MSE_test=gd_object.main()\n",
    "        train_mse.append(MSE_train)\n",
    "        test_mse.append(MSE_test)\n",
    "        \n",
    "table= pd.DataFrame({'Learning Rate' : [0.01,0.03,0.07,0.09,0.250,0.500] * 3,\n",
    "   'Epochs' : [100,500,1000] * 6,\n",
    "   'Test MSE' : test_mse,\n",
    "   'Train MSE' : train_mse})\n",
    "print(\"TABULAR DATA of MSE FOR different Epochs and Learning Rate\\n\",table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST MSE:  [0.1206, 0.0754, 0.1106, 0.1357, 0.0754, 0.1307, 0.1457, 0.0603, 0.0804, 0.1156, 0.0402, 0.0804, 0.0804, 0.1156, 0.0804, 0.0754, 0.0905, 0.1357, 0.0704, 0.1156, 0.1206, 0.1457, 0.1206, 0.1508, 0.1256, 0.0854, 0.1206, 0.1055, 0.1608, 0.1457, 0.0704, 0.0754, 0.0704, 0.1055, 0.1206, 0.1156, 0.1055, 0.1457, 0.1005, 0.1457, 0.1256, 0.1156, 0.1508, 0.0955, 0.0955, 0.1055, 0.1206, 0.1156, 0.1005, 0.1508, 0.0905, 0.0804, 0.1357, 0.1005, 0.0854, 0.1005, 0.0905, 0.0603, 0.0955, 0.0704, 0.1156, 0.1307, 0.1256, 0.0905, 0.1005, 0.1357, 0.1256, 0.1357, 0.0603, 0.1055, 0.1005, 0.0955, 0.0854, 0.1608, 0.1055, 0.1357, 0.1256, 0.1357, 0.1307, 0.0503, 0.1256, 0.1156, 0.1005, 0.0804, 0.1156, 0.0553, 0.0854, 0.0804, 0.1005, 0.0905, 0.0854, 0.1206, 0.1256, 0.1005, 0.1106, 0.1055, 0.0704, 0.1106, 0.1256, 0.1005]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 100 TEST MSE for same epochs,learning rate and stopping rulr. Displaying it in a box plot\n",
    "\n",
    "epochs=[100]\n",
    "l_rate=[0.09]\n",
    "stopping_rule=100\n",
    "TEST_MSE_all=[]\n",
    "for i in range(100):\n",
    "    for each_epoch_val in epochs:\n",
    "        for each_l_rate in l_rate:\n",
    "            gd_object=LrGd(each_l_rate ,each_epoch_val ,stopping_rule)\n",
    "            MSE_train, MSE_test=gd_object.main()\n",
    "            TEST_MSE_all.append(round(MSE_test,4))\n",
    "print(\"TEST MSE: \",TEST_MSE_all)\n",
    "df2 = pd.DataFrame({ 'TEST MSE - 100 instances' : all_values,\n",
    "               })\n",
    "plt.interactive(False)\n",
    "df2.boxplot()\n",
    "plt.title('100 Test MSE for fixed values and different intial weights')  \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
